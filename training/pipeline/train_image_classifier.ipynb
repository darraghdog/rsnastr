{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/selimsef/dfdc_deepfake_challenge/blob/master/training/pipelines/train_classifier.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "from collections import defaultdict, OrderedDict\n",
    "import platform\n",
    "PATH = '/Users/dhanley/Documents/rsnastr' \\\n",
    "        if platform.system() == 'Darwin' else '/data/rsnastr'\n",
    "os.chdir(PATH)\n",
    "sys.path.append(PATH)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import log_loss\n",
    "from utils.logs import get_logger\n",
    "from utils.utils import RSNAWEIGHTS, RSNA_CFG as CFG\n",
    "from training.tools.config import load_config\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.distributed as dist\n",
    "from training.datasets.classifier_dataset import RSNAClassifierDataset, \\\n",
    "        nSampler, valSeedSampler, collatefn\n",
    "from training.zoo import classifiers\n",
    "from training.zoo.classifiers import validate\n",
    "from training.tools.utils import create_optimizer, AverageMeter\n",
    "from training.losses import getLoss\n",
    "from training import losses\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "cv2.setNumThreads(0)\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "logger = get_logger('Train', 'INFO') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "aug = A.Compose([\n",
    "        # A.HorizontalFlip(p=1.), right/left\n",
    "        A.VerticalFlip(p=1.),\n",
    "        A.Transpose(p=0.),\n",
    "    ])\n",
    "fname = 'data/jpeg/train/4f632056046b/03dbda10118a/53ccebd24e14.jpg'\n",
    "img = cv2.imread(fname)[:,:,::-1]\n",
    "img = cv2.resize(img, (360, 360))\n",
    "from PIL import Image\n",
    "Image.fromarray(img)\n",
    "Image.fromarray(aug(image=img)['image'])\n",
    "'''\n",
    "\n",
    "import sys; sys.argv=['']; del sys\n",
    "logger.info('Load args')\n",
    "parser = argparse.ArgumentParser(\"PyTorch Xview Pipeline\")\n",
    "arg = parser.add_argument\n",
    "arg('--config', metavar='CONFIG_FILE', help='path to configuration file')\n",
    "arg('--workers', type=int, default=6, help='number of cpu threads to use')\n",
    "arg('--device', type=str, default='cpu' if platform.system() == 'Darwin' else 'cuda', help='device for model - cpu/gpu')\n",
    "arg('--gpu', type=str, default='0', help='List of GPUs for parallel training, e.g. 0,1,2,3')\n",
    "arg('--output-dir', type=str, default='weights/')\n",
    "arg('--resume', type=str, default='')\n",
    "arg('--fold', type=int, default=0)\n",
    "arg('--accum', type=int, default=1)\n",
    "arg('--batchsize', type=int, default=4)\n",
    "arg('--labeltype', type=str, default='all') # or 'single'\n",
    "arg('--augextra', type=str, default=False) # or 'single'\n",
    "arg('--mixup_beta', type=float, default = 0.)\n",
    "arg('--prefix', type=str, default='classifier_')\n",
    "arg('--data-dir', type=str, default=\"data\")\n",
    "arg('--folds-csv', type=str, default='folds.csv.gz')\n",
    "arg('--crops-dir', type=str, default='jpegip')\n",
    "arg('--label-smoothing', type=float, default=0.01)\n",
    "arg('--logdir', type=str, default='logs/b2_1820')\n",
    "arg('--distributed', action='store_true', default=False)\n",
    "arg('--freeze-epochs', type=int, default=0)\n",
    "arg(\"--local_rank\", default=0, type=int)\n",
    "arg(\"--seed\", default=777, type=int)\n",
    "arg(\"--opt-level\", default='O1', type=str)\n",
    "arg(\"--test_every\", type=int, default=1)\n",
    "arg('--from-zero', action='store_true', default=False)\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device='cuda' \n",
    "args.fold=0 \n",
    "args.accum=4 \n",
    "args.batchsize=32 \n",
    "args.logdir='logs/zoo' \n",
    "args.augextra=False  \n",
    "args.label_smoothing=0.0 \n",
    "args.config='configs/effnetb5_lr5e4_binary_accum.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    args.config = 'configs/b2.json'\n",
    "    args.config = 'configs/b2_binary.json'\n",
    "    args.config = 'configs/rnxt101_binary.json'\n",
    "conf = load_config(args.config)\n",
    "\n",
    "# Try using imagenet means\n",
    "if not args.augextra:\n",
    "    def create_train_transforms(size=300, distort = False):\n",
    "        return A.Compose([\n",
    "            #A.HorizontalFlip(p=0.5),   # right/left\n",
    "            A.VerticalFlip(p=0.5), \n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, value = 0,\n",
    "                                 rotate_limit=20, p=0.5, border_mode = cv2.BORDER_CONSTANT),\n",
    "            # A.Cutout(num_holes=40, max_h_size=size//7, max_w_size=size//7, fill_value=128, p=0.5), \n",
    "            #A.Transpose(p=0.5), # swing in -90 degrees\n",
    "            A.Resize(size, size, p=1), \n",
    "            A.Normalize(mean=conf['normalize']['mean'], \n",
    "                        std=conf['normalize']['std'], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensor()\n",
    "        ])\n",
    "else:\n",
    "    def create_train_transforms(size=300, distort = False):\n",
    "        return A.Compose([\n",
    "            #A.HorizontalFlip(p=0.5),   # right/left\n",
    "            A.VerticalFlip(p=0.5), \n",
    "            A.OneOf([\n",
    "                A.RandomCrop(int(size*0.8), int(size*0.8), p = 0.5), \n",
    "                A.RandomCrop(int(size*0.9), int(size*0.9), p = 0.5), \n",
    "            ], p=1.0),\n",
    "            A.OneOf([\n",
    "                A.ElasticTransform(p=0.5, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "                A.GridDistortion(p=0.5),\n",
    "                A.OpticalDistortion(p=1, distort_limit=2, shift_limit=0.5),\n",
    "            ], p=0.5),\n",
    "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, value = 0,\n",
    "                                 rotate_limit=20, p=0.5, border_mode = cv2.BORDER_CONSTANT),\n",
    "            # A.Cutout(num_holes=40, max_h_size=size//7, max_w_size=size//7, fill_value=128, p=0.5), \n",
    "            #A.Transpose(p=0.5), # swing in -90 degrees\n",
    "            A.Resize(size, size, p=1), \n",
    "            A.Normalize(mean=conf['normalize']['mean'], \n",
    "                        std=conf['normalize']['std'], max_pixel_value=255.0, p=1.0),\n",
    "            ToTensor()\n",
    "        ])\n",
    "\n",
    "def create_val_transforms(size=300, HFLIPVAL = 1.0, TRANSPOSEVAL = 1.0):\n",
    "    return A.Compose([\n",
    "        #A.HorizontalFlip(p=HFLIPVAL),\n",
    "        #A.Transpose(p=TRANSPOSEVAL),\n",
    "        A.Normalize(mean=conf['normalize']['mean'], \n",
    "                    std=conf['normalize']['std'], max_pixel_value=255.0, p=1.0),\n",
    "        ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Create traindatasets')\n",
    "trndataset = RSNAClassifierDataset(mode=\"train\",\n",
    "                                       fold=args.fold,\n",
    "                                       imgsize = conf['size'],\n",
    "                                       crops_dir=args.crops_dir,\n",
    "                                       imgclasses=CFG[\"image_target_cols\"],\n",
    "                                       studyclasses=CFG['exam_target_cols'],\n",
    "                                       data_path=args.data_dir,\n",
    "                                       label_smoothing=args.label_smoothing,\n",
    "                                       folds_csv=args.folds_csv,\n",
    "                                       transforms=create_train_transforms(conf['size']))\n",
    "logger.info('Create valdatasets')\n",
    "valdataset = RSNAClassifierDataset(mode=\"valid\",\n",
    "                                    fold=args.fold,\n",
    "                                    crops_dir=args.crops_dir,\n",
    "                                    imgclasses=CFG[\"image_target_cols\"],\n",
    "                                    studyclasses=CFG['exam_target_cols'],\n",
    "                                    imgsize = conf['size'],\n",
    "                                    data_path=args.data_dir,\n",
    "                                    folds_csv=args.folds_csv,\n",
    "                                    transforms=create_val_transforms(conf['size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valsampler = valSeedSampler(valdataset.data, N = 5000, seed = args.seed)\n",
    "logger.info(50*'-')\n",
    "logger.info(valdataset.data.loc[valsampler.sampler]['pe_present_on_image'].value_counts())\n",
    "loaderargs = {'num_workers' : 8, 'pin_memory': False, 'drop_last': False, 'collate_fn' : collatefn}\n",
    "valloader = DataLoader(valdataset, batch_size=args.batchsize, sampler = valsampler, **loaderargs)\n",
    "\n",
    "logger.info('Create model and optimisers')\n",
    "nclasses = len(CFG[\"image_target_cols\"]) + len(CFG['exam_target_cols'])\n",
    "model = classifiers.__dict__[conf['network']](encoder=conf['encoder'],nclasses = nclasses)\n",
    "model = model.to(args.device)\n",
    "\n",
    "'''\n",
    "reduction = \"mean\"\n",
    "losstype = list(conf['losses'].keys())[0]\n",
    "criterion = getLoss(\"BCEWithLogitsLoss\", args.device)\n",
    "'''\n",
    "bce_wts = torch.tensor([1.] + CFG['exam_weights']).to(args.device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='mean', weight = bce_wts)\n",
    "\n",
    "optimizer, scheduler = create_optimizer(conf['optimizer'], model)\n",
    "bce_best = 100\n",
    "start_epoch = 0\n",
    "batch_size = conf['optimizer']['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(args.logdir, exist_ok=True)\n",
    "summary_writer = SummaryWriter(args.logdir + '/' + conf.get(\"prefix\", args.prefix) + conf['encoder'] + \"_\" + str(args.fold))\n",
    "\n",
    "if args.from_zero:\n",
    "    start_epoch = 0\n",
    "current_epoch = start_epoch\n",
    "\n",
    "if conf['fp16'] and args.device != 'cpu':\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "snapshot_name = \"{}{}_{}_{}_\".format(conf.get(\"prefix\", args.prefix), conf['network'], conf['encoder'], args.fold)\n",
    "max_epochs = conf['optimizer']['schedule']['epochs']\n",
    "\n",
    "logger.info('Start training')\n",
    "epoch_img_names = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "alldf = pd.read_csv('data/train.csv.zip')\n",
    "allsampler = nSampler(alldf, pe_weight = 0.66, nmin = 2, nmax = 4, seed = None)\n",
    "len(allsampler.sample(alldf)) * 0.8\n",
    "'''\n",
    "seenratio=0  # Ratio of seen in images in previous epochs\n",
    "\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    '''\n",
    "    Here we took out a load of things, check back \n",
    "    https://github.com/selimsef/dfdc_deepfake_challenge/blob/9925d95bc5d6545f462cbfb6e9f37c69fa07fde3/training/pipelines/train_classifier.py#L188-L201\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    TRAIN\n",
    "    '''\n",
    "    ep_samps={'tot':0,'pos':0}\n",
    "    losses = AverageMeter()\n",
    "    max_iters = conf[\"batches_per_epoch\"]\n",
    "    trnsampler = nSampler(trndataset.data, \n",
    "                          pe_weight = conf['pe_ratio'], \n",
    "                          nmin = conf['studynmin'], \n",
    "                          nmax = conf['studynmax'], \n",
    "                          seed = None)\n",
    "    if current_epoch == 0: \n",
    "        trncts = trndataset.data.iloc[trnsampler.sample(trndataset.data)].pe_present_on_image.value_counts()\n",
    "        valcts = valdataset.data.iloc[valsampler.sample(valdataset.data)].pe_present_on_image.value_counts()\n",
    "        logger.info(f'Train class balance:\\n{trncts}')\n",
    "        logger.info(f'Valid class balance:\\n{valcts}')\n",
    "    trnloader = DataLoader(trndataset, batch_size=args.batchsize, sampler = trnsampler, **loaderargs)\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(trnloader), total=max_iters, desc=\"Epoch {}\".format(current_epoch), ncols=0)\n",
    "    if conf[\"optimizer\"][\"schedule\"][\"mode\"] == \"current_epoch\":\n",
    "        scheduler.step(current_epoch)\n",
    "    for i, sample in pbar:\n",
    "        epoch_img_names[current_epoch] += sample['img_name']\n",
    "        imgs = sample[\"image\"].to(args.device)\n",
    "        # logger.info(f'Mean {imgs.mean()} std {imgs.std()} ')\n",
    "        labels = sample[\"labels\"].to(args.device).float()\n",
    "        if conf['fp16'] and args.device != 'cpu':\n",
    "            with autocast():\n",
    "                out = model(imgs)\n",
    "                loss = criterion(out, labels) # 0.6710\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i % args.accum) == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        losses.update(loss.item(), imgs.size(0))\n",
    "        pbar.set_postfix({\"lr\": float(scheduler.get_lr()[-1]), \"epoch\": current_epoch, \n",
    "                          \"loss\": losses.avg, 'seen_prev': seenratio })\n",
    "        \n",
    "        if conf[\"optimizer\"][\"schedule\"][\"mode\"] in (\"step\", \"poly\"):\n",
    "            scheduler.step(i + current_epoch * max_iters)\n",
    "        if i == max_iters - 1:\n",
    "            break\n",
    "    pbar.close()\n",
    "    if epoch > 0:\n",
    "        seen = set(epoch_img_names[epoch]).intersection(\n",
    "            set(itertools.chain(*[epoch_img_names[i] for i in range(epoch)])))\n",
    "        seenratio = len(seen)/len(epoch_img_names[epoch])\n",
    "\n",
    "    for idx, param_group in enumerate(optimizer.param_groups):\n",
    "        lr = param_group['lr']\n",
    "        summary_writer.add_scalar('group{}/lr'.format(idx), float(lr), global_step=current_epoch)\n",
    "        summary_writer.add_scalar('train/loss', float(losses.avg), global_step=current_epoch)\n",
    "    model = model.eval()\n",
    "    # bce, acc, probdf = validate(model, valloader, device = args.device, logger = logger, half = False)\n",
    "    '''\n",
    "    Validate\n",
    "    '''\n",
    "    \n",
    "    val = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for i, sample in tqdm(enumerate(valloader)):\n",
    "            imgs = sample[\"image\"].to(args.device)\n",
    "            val['img_names'] += sample[\"img_name\"]\n",
    "            val['targets'] += sample[\"labels\"].tolist()\n",
    "            val['studype'] += sample['studype'].tolist()\n",
    "            out = model(imgs)\n",
    "            preds = torch.sigmoid(out).detach().cpu().numpy()\n",
    "            val['probs'].append(preds)\n",
    "    val['probs'] = np.concatenate(val['probs'], 0)\n",
    "    val['targets'] = np.array(val['targets']).round()*1.\n",
    "    val['studype'] = np.array(val['studype']).round()\n",
    "    val['negimg_idx'] = ((val['targets'][:,0] < 0.5) & (val['studype'] > 0.5)).flatten()\n",
    "    val['posimg_idx'] = ((val['targets'][:,0] > 0.5) & (val['studype'] > 0.5)).flatten()\n",
    "    val['negstd_idx'] = ((val['targets'][:,0] < 0.5) & (val['studype'] < 0.5)).flatten()\n",
    "    ycols = CFG['image_target_cols']+CFG['exam_target_cols']\n",
    "    ywts = [CFG['image_weight']] + CFG['exam_weights']\n",
    "    idxs = ['negimg_idx', 'posimg_idx', 'negstd_idx']\n",
    "    bce_val = torch.nn.BCELoss(reduction='mean')\n",
    "    valcriterion = torch.nn.BCEWithLogitsLoss(reduction='mean', \n",
    "                                              weight = bce_wts.to('cpu'))\n",
    "    outtmp = torch.tensor(val['probs'])\n",
    "    ytmp = torch.tensor(val['targets']*1.)\n",
    "    bce = valcriterion(outtmp, ytmp)\n",
    "    for idx in idxs:\n",
    "        outtmp = torch.tensor(val['probs'][val[idx], :])\n",
    "        ytmp = torch.tensor(val['targets'][val[idx], :]).float()\n",
    "        lossvaltmp = bce_val(outtmp, ytmp)\n",
    "        logger.info(f'idx {idx} loss {lossvaltmp:.3f}')\n",
    "    print(50*'--')\n",
    "    for i, (col, wt) in enumerate(zip(ycols, ywts)):\n",
    "        idx = 'posimg_idx'\n",
    "        outtmp = torch.tensor(val['probs'][val[idx], i])\n",
    "        ytmp = torch.tensor(val['targets'][val[idx], i]).float()\n",
    "        lossvaltmp = bce_val(outtmp, ytmp)\n",
    "        logger.info(f'Type {col} '.ljust(27)+f'wt {wt:.3f} idx {idx} loss {lossvaltmp:.3f}')\n",
    "    print(50*'--')\n",
    "    for idx in idxs:\n",
    "        col = CFG['image_target_cols'][0]\n",
    "        wt = CFG['image_weight']\n",
    "        outtmp = torch.tensor(val['probs'][val[idx], :])\n",
    "        ytmp = torch.tensor(val['targets'][val[idx], :]).float()\n",
    "        lossvaltmp = bce_val(outtmp, ytmp)\n",
    "        logger.info(f'Type {col} '.ljust(27)+f'idx {idx} loss {lossvaltmp:.3f}')\n",
    "    print(50*'--')   \n",
    "    \n",
    "    '''\n",
    "    Save the model\n",
    "    '''\n",
    "\n",
    "    if args.local_rank == 0:\n",
    "        if bce < bce_best:\n",
    "            print(\"Epoch {} improved from {:.5f} to {:.5f}\".format(current_epoch, bce_best, bce))\n",
    "            if args.output_dir is not None:\n",
    "                torch.save({\n",
    "                    'epoch': current_epoch + 1,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'bce_best': bce,\n",
    "                }, args.output_dir + snapshot_name + f\"_fold{args.fold}_best_dice___testme\")\n",
    "            bce_best = bce\n",
    "        print(\"Epoch: {} bce: {:.5f}, bce_best: {:.5f}\".format(current_epoch, bce, bce_best))\n",
    "    torch.save({\n",
    "        'epoch': current_epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'bce_best': bce,\n",
    "        }, args.output_dir + snapshot_name + f\"_fold{args.fold}_epoch{current_epoch}\")\n",
    "    current_epoch += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
