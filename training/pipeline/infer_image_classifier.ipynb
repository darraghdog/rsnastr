{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/selimsef/dfdc_deepfake_challenge/blob/master/training/pipelines/train_classifier.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n",
    "import itertools\n",
    "from collections import defaultdict, OrderedDict\n",
    "import platform\n",
    "import glob\n",
    "PATH = '/Users/dhanley/Documents/rsnastr' \\\n",
    "        if platform.system() == 'Darwin' else '/data/rsnastr'\n",
    "os.chdir(PATH)\n",
    "sys.path.append(PATH)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import log_loss\n",
    "from utils.logs import get_logger\n",
    "#from utils.swa_utils import swa\n",
    "from utils.utils import RSNAWEIGHTS\n",
    "from training.tools.config import load_config\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from utils.swa_utils import swa\n",
    "\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import DataParallel\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "from training.datasets.classifier_dataset import RSNAClassifierDataset, \\\n",
    "        nSampler, valSeedSampler, collatefn\n",
    "from training.zoo import classifiers\n",
    "from training.zoo.sequence import StudyImgNet\n",
    "from training.zoo.classifiers import swa_update_bn, validate\n",
    "\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "cv2.setNumThreads(0)\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "logger = get_logger('Train', 'INFO') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "logger.info('Load args')\n",
    "parser = argparse.ArgumentParser(\"PyTorch Xview Pipeline\")\n",
    "arg = parser.add_argument\n",
    "arg('--config', metavar='CONFIG_FILE', help='path to configuration file')\n",
    "arg('--workers', type=int, default=6, help='number of cpu threads to use')\n",
    "arg('--type', type=str, default='image', help='Image model of study model')\n",
    "arg(\"--seed\", default=777, type=int)\n",
    "arg('--device', type=str, default='cpu' if platform.system() == 'Darwin' else 'cuda', help='device for model - cpu/gpu')\n",
    "arg('--gpu', type=str, default='0', help='List of GPUs for parallel training, e.g. 0,1,2,3')\n",
    "arg('--output-dir', type=str, default='weights/')\n",
    "arg('--weightsrgx', type=str, default='classifier_RSNAClassifier_resnext101_32x8d_0__fold0_epoch2*')\n",
    "arg('--epochs', type=str, default='21|22|23')\n",
    "arg('--fold', type=int, default=0)\n",
    "arg('--runswa', default=False, type=lambda x: (str(x).lower() == 'true'))\n",
    "arg('--infer', default=False, type=lambda x: (str(x).lower() == 'true'))\n",
    "arg('--emb', default=False, type=lambda x: (str(x).lower() == 'true'))\n",
    "arg('--batchsize', type=int, default=4)\n",
    "arg('--concatsteps', type=int, default=32)\n",
    "arg('--labeltype', type=str, default='all') \n",
    "arg('--prefix', type=str, default='classifier_')\n",
    "arg('--data-dir', type=str, default=\"data\")\n",
    "arg('--folds-csv', type=str, default='folds.csv.gz')\n",
    "arg('--crops-dir', type=str, default='jpegip')\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device='cuda' \n",
    "args.fold=0\n",
    "args.batchsize=64\n",
    "args.config='configs/effnetb2_lr5e4_multi.json'\n",
    "args.type = 'study'\n",
    "args.weightsrgx = 'exam_lstm_tf_efficientnet_b2_ns_epoch31_fold4.bin*'\n",
    "args.emb=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(args)\n",
    "HFLIP = False\n",
    "TRANSPOSE = False\n",
    "\n",
    "if False:\n",
    "    args.config = 'configs/rnxt101_binary.json'\n",
    "conf = load_config(args.config)\n",
    "\n",
    "# Try using imagenet means\n",
    "def create_val_transforms(size=300, HFLIPVAL = 1.0, TRANSPOSEVAL = 1.0):\n",
    "    return A.Compose([\n",
    "        A.Normalize(mean=conf['normalize']['mean'], \n",
    "                    std=conf['normalize']['std'], max_pixel_value=255.0, p=1.0),\n",
    "        ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('Create valdatasets')\n",
    "valdataset = RSNAClassifierDataset(mode=\"valid\",\n",
    "                                         fold=args.fold,\n",
    "                                         crops_dir=args.crops_dir,\n",
    "                                         imgclasses=conf[\"image_target_cols\"],\n",
    "                                         studyclasses=conf['exam_target_cols'],\n",
    "                                         imgsize = conf['size'],\n",
    "                                         data_path=args.data_dir,\n",
    "                                         folds_csv=args.folds_csv,\n",
    "                                         transforms=create_val_transforms(conf['size']))\n",
    "alldataset = RSNAClassifierDataset(mode=\"all\",\n",
    "                                           fold=args.fold,\n",
    "                                           imgsize = conf['size'],\n",
    "                                           crops_dir=args.crops_dir,\n",
    "                                           imgclasses=conf[\"image_target_cols\"],\n",
    "                                           studyclasses=conf['exam_target_cols'],\n",
    "                                           data_path=args.data_dir,\n",
    "                                           label_smoothing=0.00,\n",
    "                                           folds_csv=args.folds_csv,\n",
    "                                           transforms=create_val_transforms(conf['size']))\n",
    "valsampler = valSeedSampler(valdataset.data, N = 5000, seed = args.seed)\n",
    "logger.info(50*'-')\n",
    "logger.info(valdataset.data.loc[valsampler.sampler]['pe_present_on_image'].value_counts())\n",
    "loaderargs = {'num_workers' : 8, 'pin_memory': False, 'drop_last': False, 'collate_fn' : collatefn}\n",
    "valloader = DataLoader(valdataset, batch_size=args.batchsize, sampler = valsampler, **loaderargs)\n",
    "allloader = DataLoader(alldataset, batch_size=args.batchsize, shuffle=False, **loaderargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightfiles = glob.glob(f'{args.output_dir}/{args.weightsrgx}')\n",
    "epochs = list(map(lambda x: f'_epoch{x}', args.epochs.split('|')))\n",
    "#weightfiles = [w for w in weightfiles if any(e in w for e in epochs)]\n",
    "logger.info(f'Weights to process: {weightfiles}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.emb:\n",
    "    for f in weightfiles:\n",
    "        logger.info(f'Infer {f}')\n",
    "        if args.type=='image':\n",
    "            model = classifiers.__dict__[conf['network']](encoder=conf['encoder'], \\\n",
    "                                                  nclasses = len(conf['classes']),\n",
    "                                                  infer=True)\n",
    "            checkpoint = torch.load(f, map_location=torch.device(args.device))\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        if args.type=='study':\n",
    "            nc = len(conf['image_target_cols']+conf['exam_target_cols'])\n",
    "            model =StudyImgNet(conf['encoder'], \n",
    "                               dropout = 0.0,\n",
    "                               nclasses = nc,\n",
    "                               dense_units = 512)\n",
    "            checkpoint = torch.load(f, map_location=torch.device(args.device))\n",
    "            model.load_state_dict(checkpoint)\n",
    "            model = model.encoder\n",
    "        model = model.half().to(args.device)\n",
    "        model = model.eval()\n",
    "        logger.info(f'Embeddings total : {len(allloader)}')\n",
    "        pbar = tqdm(enumerate(allloader), total=len(allloader), desc=\"Weights {}\".format(f), ncols=0)\n",
    "        embls = []\n",
    "        img_names = []\n",
    "        with torch.no_grad():\n",
    "            for i, sample in pbar:\n",
    "                img_names += sample['img_name']\n",
    "                imgs = sample[\"image\"].half().to(args.device)\n",
    "                emb = model(imgs)\n",
    "                embls.append(emb.detach().cpu().numpy().astype(np.float32))\n",
    "        outemb = np.concatenate(embls)\n",
    "        logger.info('Write embeddings : shape {} {}'.format(*outemb.shape))\n",
    "        fembname =  f'{f}__all_size{conf[\"size\"]}.emb'\n",
    "        #fembname = 'emb/'+fembname.replace(args.output_dir, '')\n",
    "        logger.info('Embedding file name : {}'.format(fembname))\n",
    "        np.savez_compressed(os.path.join('emb', fembname), outemb)\n",
    "        valdataset.data.to_pickle( f'emb/{fembname}.data.pk' )\n",
    "        with open(f'emb/{fembname}.imgnames.pk', 'wb') as handle:\n",
    "            pickle.dump(img_names, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.encoder(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
