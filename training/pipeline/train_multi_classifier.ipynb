{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/selimsef/dfdc_deepfake_challenge/blob/master/training/pipelines/train_classifier.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "import itertools\n",
    "from collections import defaultdict, OrderedDict\n",
    "import platform\n",
    "PATH = '/Users/dhanley/Documents/rsnastr' \\\n",
    "        if platform.system() == 'Darwin' else '/data/rsnastr'\n",
    "os.chdir(PATH)\n",
    "sys.path.append(PATH)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import log_loss\n",
    "from utils.logs import get_logger\n",
    "from utils.utils import RSNAWEIGHTS, RSNA_CFG as CFG\n",
    "from training.tools.config import load_config\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.backends import cudnn\n",
    "from torch.nn import DataParallel\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.distributed as dist\n",
    "from training.datasets.classifier_dataset import RSNAImageSequenceDataset, collateseqimgfn\n",
    "from training.zoo.sequence import StudyImgNet\n",
    "from training.tools.utils import create_optimizer, AverageMeter, collectPreds, collectLoss\n",
    "from training.tools.utils import splitbatch, unmasklabels, unmasklogits\n",
    "from training.losses import getLoss\n",
    "from training import losses\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "cv2.setNumThreads(0)\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensor\n",
    "logger = get_logger('LSTM', 'INFO') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_transforms_multi(size=300, distort = False):\n",
    "    return A.Compose([\n",
    "        #A.HorizontalFlip(p=0.5),   # right/left\n",
    "        #A.VerticalFlip(p=0.5), \n",
    "        A.ShiftScaleRotate(shift_limit=0.02, scale_limit=0.02, value = 0,\n",
    "                                 rotate_limit=10, p=0.5, border_mode = cv2.BORDER_CONSTANT),\n",
    "        # A.Cutout(num_holes=40, max_h_size=size//7, max_w_size=size//7, fill_value=128, p=0.5), \n",
    "        #A.Transpose(p=0.5), # swing in -90 degrees\n",
    "        A.Resize(size, size, p=1), \n",
    "        A.Normalize(mean=conf['normalize']['mean'], \n",
    "                    std=conf['normalize']['std'], max_pixel_value=255.0, p=1.0),\n",
    "        ToTensor()\n",
    "        ])\n",
    "\n",
    "def create_val_transforms(size=300, HFLIPVAL = 1.0, TRANSPOSEVAL = 1.0):\n",
    "    return A.Compose([\n",
    "        #A.HorizontalFlip(p=HFLIPVAL),\n",
    "        #A.Transpose(p=TRANSPOSEVAL),\n",
    "        A.Normalize(mean=conf['normalize']['mean'], \n",
    "                    std=conf['normalize']['std'], max_pixel_value=255.0, p=1.0),\n",
    "        ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-11 20:16:52,928 - LSTM - INFO - Load args\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "logger.info('Load args')\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "arg = parser.add_argument\n",
    "arg('--config', metavar='CONFIG_FILE', help='path to configuration file')\n",
    "arg('--workers', type=int, default=6, help='number of cpu threads to use')\n",
    "arg('--device', type=str, default='cpu' if platform.system() == 'Darwin' else 'cuda', help='device for model - cpu/gpu')\n",
    "arg('--gpu', type=str, default='0', help='List of GPUs for parallel training, e.g. 0,1,2,3')\n",
    "arg('--output-dir', type=str, default='weights/')\n",
    "arg('--resume', type=str, default='')\n",
    "arg('--fold', type=int, default=0)\n",
    "arg('--batchsize', type=int, default=1)\n",
    "arg('--lr', type=float, default = 0.0001)\n",
    "arg('--lrgamma', type=float, default = 0.98)\n",
    "arg('--labeltype', type=str, default='all') # or 'single'\n",
    "arg('--dropout', type=float, default = 0.2)\n",
    "arg('--prefix', type=str, default='classifier_')\n",
    "arg('--data-dir', type=str, default=\"data\")\n",
    "arg('--folds-csv', type=str, default='folds.csv.gz')\n",
    "arg('--nclasses', type=str, default=1)\n",
    "arg('--crops-dir', type=str, default='jpegip')\n",
    "arg('--lstm_units',   type=int, default=512)\n",
    "arg('--epochs',   type=int, default=12)\n",
    "arg('--nbags',   type=int, default=12)\n",
    "arg('--accum', type=int, default=16)\n",
    "arg('--label-smoothing', type=float, default=0.00)\n",
    "arg('--logdir', type=str, default='logs/b2_1820')\n",
    "arg(\"--local_rank\", default=0, type=int)\n",
    "arg('--embrgx', type=str, default='weights/image_weights_regex')\n",
    "arg(\"--seed\", default=777, type=int)\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device='cuda' \n",
    "args.fold=0 \n",
    "args.epochs=32\n",
    "args.config='configs/effnetb2_lr5e4_multi.json'\n",
    "conf = load_config(args.config)\n",
    "conf['sample_count'] = 128\n",
    "conf['balanced'] = False # True\n",
    "conf['image_weight'] = 0.07361963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf['encoder'] = 'tf_efficientnet_b0_ns'\n",
    "conf['learning_rate'] = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-11 20:16:52,950 - LSTM - INFO - Create traindatasets\n",
      "2020-10-11 20:16:56,402 - LSTM - INFO - Create valdatasets\n"
     ]
    }
   ],
   "source": [
    "logger.info('Create traindatasets')\n",
    "trndataset = RSNAImageSequenceDataset(mode=\"train\",\\\n",
    "                                       fold=args.fold,\\\n",
    "                                       pos_sample_weight = conf['pos_sample_weight'],\\\n",
    "                                       sample_count = conf['sample_count'], \\\n",
    "                                       imgsize = conf['size'],\\\n",
    "                                       crops_dir=args.crops_dir,\\\n",
    "                                       balanced=conf['balanced'],\\\n",
    "                                       imgclasses=conf[\"image_target_cols\"],\\\n",
    "                                       studyclasses=conf['exam_target_cols'],\\\n",
    "                                       data_path=args.data_dir,\\\n",
    "                                       label_smoothing=args.label_smoothing,\\\n",
    "                                       folds_csv=args.folds_csv,\\\n",
    "                                       transforms=create_train_transforms_multi(conf['size'])\\\n",
    "                                           if len(conf['exam_target_cols'])>0 else \\\n",
    "                                           create_train_transforms_binary(conf['size']))\n",
    "logger.info('Create valdatasets')\n",
    "valdataset = RSNAImageSequenceDataset(mode=\"valid\",\n",
    "                                    fold=args.fold,\n",
    "                                    pos_sample_weight = conf['pos_sample_weight'],\n",
    "                                    sample_count = conf['sample_count'], \n",
    "                                    crops_dir=args.crops_dir,\n",
    "                                    balanced=conf['balanced'],\n",
    "                                    imgclasses=conf[\"image_target_cols\"],\n",
    "                                    studyclasses=conf['exam_target_cols'],\n",
    "                                    imgsize = conf['size'],\n",
    "                                    data_path=args.data_dir,\n",
    "                                    folds_csv=args.folds_csv,\n",
    "                                    transforms=create_val_transforms(conf['size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-11 20:16:59,219 - LSTM - INFO - Create loaders...\n",
      "2020-10-11 20:16:59,288 - LSTM - INFO - Create model\n"
     ]
    }
   ],
   "source": [
    "logger.info('Create loaders...')\n",
    "def sampler(dataset):\n",
    "    wts = dataset.folddf.negative_exam_for_pe.values\n",
    "    w0 = (wts>0.5).sum()\n",
    "    w1 = (wts<0.5).sum()\n",
    "    wts[wts==1] = w1\n",
    "    wts[wts==0] = w0\n",
    "    sampler = WeightedRandomSampler(wts, len(wts), replacement=True)\n",
    "    return sampler\n",
    "    \n",
    "valloader = DataLoader(valdataset, \n",
    "                       batch_size=args.batchsize, \n",
    "                       sampler=sampler(valdataset), \n",
    "                       num_workers=8, \n",
    "                       collate_fn=collateseqimgfn)\n",
    "# del embmat\n",
    "gc.collect()\n",
    "\n",
    "logger.info('Create model')\n",
    "nc = len(conf['image_target_cols']+conf['exam_target_cols'])\n",
    "model =StudyImgNet(conf['encoder'], \n",
    "                   dropout = 0.2,\n",
    "                   nclasses = nc,\n",
    "                   dense_units = 512)\n",
    "'''\n",
    "batch = next(iter(trnloader))\n",
    "x = batch['image']\n",
    "out = model(x)\n",
    "'''\n",
    "model = model.to(args.device)\n",
    "DECAY = 0.0\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "plist = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': DECAY},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "optimizer = torch.optim.Adam(plist, lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=args.lrgamma, last_epoch=-1)\n",
    "\n",
    "# Exam Loss\n",
    "bcewLL_func = torch.nn.BCEWithLogitsLoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypredls = []\n",
    "ypredtstls = []\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-10-11 20:17:02,187 - LSTM - INFO - Start training\n",
      "2020-10-11 20:17:02,189 - LSTM - INFO - --------------------------------------------------\n",
      "Train epoch 0:  29% 1674/5823 [10:55<25:51,  2.67it/s, train loss=0.64, image loss=0.64, exam loss=0.608]  "
     ]
    }
   ],
   "source": [
    "logger.info('Start training')\n",
    "for epoch in range(args.epochs):\n",
    "    trnloader = DataLoader(trndataset, \n",
    "                       batch_size=args.batchsize, \n",
    "                       sampler=sampler(trndataset), \n",
    "                       num_workers=8, \n",
    "                       collate_fn=collateseqimgfn)\n",
    "    logger.info(50*'-')\n",
    "    trnloss   = 0.\n",
    "    trnwts    = 0.\n",
    "    trnimgloss   = 0.\n",
    "    trnimgwts    = 0.\n",
    "    trnexmloss   = 0.\n",
    "    trnexmwts    = 0.\n",
    "    label_w = torch.tensor(conf['exam_weights']).to(args.device, dtype=torch.float)\n",
    "    img_w = torch.tensor(conf['image_weight']).to(args.device, dtype=torch.float)\n",
    "    model = model.train()\n",
    "    pbar = tqdm(enumerate(trnloader), \n",
    "                total = len(trndataset)//trnloader.batch_size, \n",
    "                desc=f\"Train epoch {epoch}\", ncols=0)\n",
    "    for step, batch in pbar:\n",
    "        ytrn = batch['labels'].to(args.device, dtype=torch.float)\n",
    "        xtrn = batch['image'].to(args.device, dtype=torch.float)\n",
    "        xtrn = torch.autograd.Variable(xtrn, requires_grad=True)\n",
    "        ytrn = torch.autograd.Variable(ytrn)\n",
    "        ytrn = ytrn.view(-1, 10)\n",
    "        #logger.info(xtrn.shape)\n",
    "        with autocast():\n",
    "            outimg, outexm = model(xtrn)\n",
    "            # Exam loss\n",
    "            exam_loss = bcewLL_func(outexm, ytrn[:1,1:])\n",
    "            exam_loss = torch.sum(exam_loss*label_w, 1)[0]\n",
    "            # Image loss\n",
    "            y_pred_img_ = outimg.squeeze(-1)\n",
    "            y_true_img_ = ytrn[:,:1].transpose(0,1)\n",
    "            image_loss = bcewLL_func(y_pred_img_, y_true_img_)\n",
    "            img_num = y_pred_img_.shape[-1]\n",
    "            qi = torch.sum(y_true_img_)\n",
    "            image_loss = torch.sum(img_w*qi*image_loss)\n",
    "            # Sum it all\n",
    "            samploss = exam_loss+image_loss\n",
    "            sampwts = label_w.sum() + (img_w*qi*img_num)\n",
    "        loss = (samploss/sampwts) / args.accum\n",
    "        scaler.scale(loss).backward()\n",
    "        trnloss += samploss.item()\n",
    "        trnwts += sampwts.item()\n",
    "        trnimgloss   += image_loss.item()\n",
    "        trnimgwts    += (img_w*qi*img_num).item()\n",
    "        trnexmloss   += exam_loss.item()\n",
    "        trnexmwts    += label_w.sum().item()\n",
    "        # logger.info(f'{image_loss.item():.4f}\\t{(img_w*qi*img_num).item():.4f}\\t{exam_loss.item():.4f}\\t{label_w.sum().item():.4f}\\t')\n",
    "        if (step+1) % args.accum == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "        final_trn_loss = trnloss/trnwts\n",
    "        pbar.set_postfix({'train loss': final_trn_loss, \n",
    "                          'image loss': trnimgloss/trnimgwts, \n",
    "                          'exam loss': trnexmloss/trnexmwts})\n",
    "        del xtrn, ytrn, outimg, outexm\n",
    "        if step%100==0:\n",
    "            torch.cuda.empty_cache()    \n",
    "    logger.info(f'Epoch {epoch} train loss all {final_trn_loss:.4f}')\n",
    "    output_model_file = f'weights/exam_lstm_{conf[\"encoder\"]}_epoch{epoch}_fold{args.fold}.bin'\n",
    "    torch.save(model.state_dict(), output_model_file)\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    valloss   = 0.\n",
    "    valwts    = 0.\n",
    "    ypredls = []\n",
    "    yvalls = []\n",
    "    pbarval = tqdm(enumerate(valloader), \n",
    "                   total = len(valdataset)//valloader.batch_size, \n",
    "                   desc=\"Train epoch {}\".format(epoch), ncols=0)\n",
    "    for step, batch in pbarval:\n",
    "        y = batch['labels'].to(args.device, dtype=torch.float)\n",
    "        x = batch['image'].to(args.device, dtype=torch.float)\n",
    "        #logger.info(xval.shape)\n",
    "        y = y.view(-1, 10)\n",
    "        with torch.no_grad():\n",
    "            outimg, outexm = model(x)\n",
    "            # Exam loss\n",
    "            exam_loss = bcewLL_func(outexm, y[:1,1:])\n",
    "            exam_loss = torch.sum(exam_loss*label_w, 1)[0]\n",
    "            # Image loss\n",
    "            y_pred_img_ = outimg.squeeze(-1)\n",
    "            y_true_img_ = y[:,:1].transpose(0,1)\n",
    "            image_loss = bcewLL_func(y_pred_img_, y_true_img_)\n",
    "            img_num = y_pred_img_.shape[-1]\n",
    "            qi = torch.sum(y_true_img_)\n",
    "            image_loss = torch.sum(img_w*qi*image_loss)\n",
    "            # Sum it all\n",
    "            samploss = exam_loss+image_loss\n",
    "            sampwts = label_w.sum() + img_w*qi*img_num\n",
    "        valloss += samploss.item()\n",
    "        valwts += sampwts.item()\n",
    "        final_val_loss = valloss/valwts\n",
    "        pbarval.set_postfix({'valid loss': final_val_loss})\n",
    "        del x, y, outimg, outexm\n",
    "        torch.cuda.empty_cache()\n",
    "    logger.info(f'Epoch {epoch} valid loss all {final_val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnimgloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
